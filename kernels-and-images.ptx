<section xml:id="section-kernels-and-images">
  <title>Kernels and Images</title>
  <subsection xml:id="subsection-row-and-column-spaces">
    <title>Row and Column Spaces</title>
    <definition>
      <statement>
        <p>
          Let <m>M</m> be a <m>m \times n</m> matrix. The
          <term>rowspace</term> of the matrix is the span of the row
          of the matrix, thought of as vectors in <m>\RR^n</m>. It is
          a subspace of <m>\RR^n</m>. The <term>columnspace</term> of
          the matrix is the span of the columns of the matrix, thought
          of as vectors in <m>\RR^m</m>. It is a subspace of
          <m>\RR^m</m>.
        </p>
      </statement>
    </definition>
    <proposition>
      <statement>
        <p>
          Let <m>M</m> be a <m>m \times n</m> matrix. The columnspace
          and the row spaces of <m>M</m> are linear subspaces (of
          <m>\RR^m</m> and <m>\RR^n</m>, respectively) with dimension
          equal to the rank of <m>M</m>.
        </p>
      </statement>
    </proposition>
  </subsection>
  <subsection xml:id="subsection-images">
    <title>Images</title>
    <p>
      In <xref ref="section-transformations-of-spans-and-loci" />, we
      defined the image of a matrix or transformation to be the whole
      collection of outputs. (This is what we call the range in single
      variable functions.) There is a nice proposition which tells us
      how to calculate images for linear transformations.
    </p>
    <proposition>
      <statement>
        <p>
          The image of a matrix <m>M</m> is the same as its
          columnspace.
        </p>
      </statement>
    </proposition>
    <proof>
      <p>
        The image is the set of all outputs of <m>M</m>, acting on the
        whole space <m>\RR^n</m>. We can think of <m>\RR^n</m> as the
        span of its standard basis <m>e_1, e_2, \ldots e_n</m>. The
        image, then, is the span of <m>Me_1, Me_2, \ldots Me_n</m>.
        These vectors are precisely the columns of the matrix, so the
        their span is the columnspace.
      </p>
    </proof>
    <example>
      <statement>
        <p>
          Look at a <m>3 \times 3</m> vector acting on the standard
          basis of <m>\RR^3</m> to see how the images of the basis
          vectors are the columns of the matrix.
          <md>
            <mrow>
              \begin{pmatrix} -2 \amp -2 \amp 0 \\ 7 \amp -3 \amp 2 \\
              -4 \amp -1 \amp 2 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \\
              0 \end{pmatrix} = \begin{pmatrix} -2 \\ 7 \\ -4
              \end{pmatrix}
            </mrow>
            <mrow>
              \begin{pmatrix} -2 \amp -2 \amp 0 \\ 7 \amp -3 \amp 2 \\
              -4 \amp -1 \amp 2 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \\
              0 \end{pmatrix} = \begin{pmatrix} -2 \\ -3 \\ -1
              \end{pmatrix}
            </mrow>
            <mrow>
              \begin{pmatrix} -2 \amp -2 \amp 0 \\ 7 \amp -3 \amp 2 \\
              -4 \amp -1 \amp 2 \end{pmatrix} \begin{pmatrix} 0 \\ 0 \\
              1 \end{pmatrix} = \begin{pmatrix} 0 \\ 2 \\ 2
              \end{pmatrix}
            </mrow>
          </md>
        </p>
      </statement>
    </example>
  </subsection>
  <subsection xml:id="subsection-kernels">
    <title>Kernels</title>
    <definition>
      <statement>
        <p>
          Let <m>M</m> be a <m>m \times n</m> matrix representing a
          transformation <m>\RR^n \rightarrow \RR^m</m>. For any <m>u
          \in \RR^m</m>, the <term>preimage</term> of <m>u</m> is the
          set of all vectors in <m>\RR^m</m> which are mapped to
          <m>u</m> by <m>M</m>. It is writtem <m>M^{-1}\{u\}</m>.
          The <term>kernel</term> or <term>nullspace</term> of
          <m>M</m> is all vectors in <m>\RR^n</m> which are sent to
          the zero vector under the transformation associated to
          <m>M</m>, i.e., the preimage of the origin.
        </p>
      </statement>
    </definition>
    <proposition>
      <statement>
        <p>
          The preimage of <m>u</m> is the solution space of the system
          of equations associated to the extended matrix <m>(M|u)</m>
          where we add the vector <m>u</m> as a column to the matrix
          <m>M</m>.
        </p>
      </statement>
    </proposition>
    <proof>
      <p>
        The preimage is all vectors <m>v</m> in <m>\RR^n</m> that are
        sent to <m>u</m>. If we write <m>v</m> in coordinates <m>x_1,
        \ldots, x_n</m>, and apply the matrix action, we get a system
        of <m>m</m> equations with the entries of <m>u</m> as the
        constants. The matrix encoding of this system is precisely
        the extended matrix <m>(M|u)</m>.
      </p>
    </proof>
    <example>
      <statement>
        <p>
          We can look at <m>\RR^2</m> to build our intuition.
          <me>
            \begin{pmatrix} a \amp b \\ c \amp d \end{pmatrix}
            \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} x_0
            \\ y_0 \end{pmatrix}
          </me>
        </p>
        <p>
          If we write this out in terms of coordinates, we get
          <md>
            <mrow>
              ax + by \amp  = x_0
            </mrow>
            <mrow>
              cx + dy \amp  = y_0
            </mrow>
          </md>
        </p>
        <p>
          This is the system associated to the extended matrix with
          <m>\begin{pmatrix} x_0 \\ y_0 \end{pmatrix}</m> as constants.
        </p>
      </statement>
    </example>
  </subsection>
  <subsection xml:id="subsection-loci-preimages-systems">
    <title>A Central Connection Idea</title>
    <p>
      At the point int he course, we have talked about loci, solved
      linear systems, and now defined the preimage of a vector under a
      transformation. Loci are very geometric, systems are very
      algebraic, and preimages are again geometric but from a
      different perspective (transformations instead of static
      objects). Despite their different starting point, these three
      are essentially all talking about the same thing. The connection
      between all three is a major conceptual goal in this course; if
      we can understand how these three novel and abstract definition
      fit together, that points to a deep and thorough understanding
      of the three concepts. Let me try to describe this connection.
    </p>
    <p>
      Let me start with a linear system. This is a set of linear
      equation, implicitly asking what values of the variables (if
      any) solves all the equations simultaneously. To move to loci,
      all we have to do is thing geometrically. A locus is the set of
      points which satisfy some equations (linear equations in this
      course). So a locus is just the solution of a system, seen
      geometrically. A locus is a solution space. We made this
      connection before when we calcluated dimension of loci: the
      dimension was the number of free parameters in the solution of
      the associated system.
    </p>
    <p>
      So what about pre-images. Conisider a <m>m \times n</m> matrix
      <m>M</m>, representing a transformation <m>\RR^m \rightarrow
      \RR^n</m>. For a vector <m>u</m> in <m>\RR^n</m>, we can ask for
      the preimage <m>M^{-1}( \{ u\} )</m>. This is all the vectors in
      <m>\RR^m</m> which the matrix send to <m>u</m> (which could
      possibly be empty if none of the vectors are sent to <m>u</m>.
      We can express this as a matrix equation: <m>v</m> is in the
      preimage only if 
      <me>
        Mv = u
      </me>
      But then I can look at the coordinates of this equation. Each
      coordinate gives me a linear equation, so I get a system of
      <m>m</m> linear equations (because <m>v \in \RR^m</m>, so it has
      <m>m</m> coordinates). The constants of this linear system are
      the components of <m>u</m>. The matrix of this linear system is
      the extended matrix <m>(M|u)</m>. The solution to this linear
      system can be interpreted as a locus. This locus (line, plane,
      etc) it precisely all the points that <m>M</m> sends to
      <m>u</m>. In this way, solutions spaces to systems, loci, and
      preimage are all the same things and are all found by the matrix
      <m>(M|u)</m>.
    </p>
    <p>
      Specifically, the kernel is the linear space of all vector
      <m>v</m> with <m>Mv = 0</m>. This is a solution space for the
      linear system with matrix <m>(M|0)</m>, where the column after
      the dividing line is all zeroes. To calcluate a kernel, we have
      to solve a system where the constants are all zero. The matrix
      of that system is just <m>M</m>, the matrix of the
      transformation.
    </p>
  </subsection>
</section>
