<section xml:id="section-orthogonality">
  <title>Orthogonality and Symmetry</title>
  <subsection xml:id="subsection-symmetry-again">
    <title>Symmetry, Again</title>
    <p>
      A major theme of this course is symmetry. When I defined linear
      transformations, I defined them to be transformations that
      preserved something. Geometrically, they preserved linear
      subspaces: each linear subspace is sent to another linear
      subspace. Algebraically, they preserved the linear operations of
      addition and scalar multiplication: I can do vector addition or
      scalar multiplication before or after the transformation, and the
      result is the same. This the general notion of symmetry that I
      want to develop in this course: transformations that preserve
      something (either algebraic or geometric). 
    </p>
    <p>
      This general idea of symmetry continues to develop as the course
      moves deeper into the discipline of linear algebra. The
      classification of matrices, at least geometrically, is almost
      always a question of symmetry.  In this section, I'm going to
      define some new classes of matrices in terms of their symmetries
      <emdash/> in terms of what they preserve.
    </p>
    <p>
      Recall the notation I've already established for matrix sets
      and groups (with real coefficients).
      <ul>
        <li>
          <p>
            The set of <m>n \times m</m> matrices is written
            <m>M_{n,m}(\RR)</m>.
          </p>
        </li>
        <li>
          <p>
            The set of square (<m>n \times n</m>) matrices is written
            <m>M_n(\RR)</m>.
          </p>
        </li>
        <li>
          <p>
            The group of invertible matrices is called the general
            linear group and written <m>GL_n(\RR)</m> or
            <m>GL(n)</m>.
          </p>
        </li>
      </ul>
    </p>
  </subsection>
  <subsection xml:id="subsection-determinant-one">
    <title>Transformations that Preserve Size and Orientation</title>
    <p>
      Determinants measure two geometric properties: size and
      orientation. These are both potential symmetries. If a matrix
      has determinant <m>\pm 1</m>, I know that it preserves size. If
      a matrix has positive determinant, I know that it
      preserves orientation. If a matrix has determinant one,
      it preserves both.
    </p>
    <definition>
      <statement>
        <p>
          The group of matrices of determinant one with real
          coefficients is called the <term>Special Linear Group</term>
          and is written <m>SL_n(\RR)</m> or <m>SL(n)</m>.
        </p>
      </statement>
    </definition>
  </subsection>
  <subsection xml:id="subsection-orthogonal-matrices">
    <title>Preserving Lengths and Angles</title>
    <definition>
      <statement>
        <p>
          An <m>n \times n</m> matrix <m>A</m> is called
          <term>orthogonal</term> if it preserves lengths. That is,
          for all <m>v \in \RR^n</m>, <m>|v| = |Av|</m>.
        </p>
      </statement>
    </definition>
    <p>
      Preservation of lengths is not the only way to define orthogonal
      matrices. The following proposition shows the rich structure of
      this group of matrices. Before stating the proposition, here is
      a useful definition.
    </p>
    <definition>
      <statement>
        <p>
          Let <m>i,j</m> be two indices which range from 1 to
          <m>n</m>. The <term>Kronecker delta</term> is a
          twice-indexed set of numbers <m>\delta_{ij}</m>, which
          evaluates to <m>0</m> whenever <m>i \neq j</m> and <m>1</m>
          whenever <m>i=j</m>.
          <me>
            \delta_{ij} = \begin{cases} 0 \amp i \neq j \\ 1
            \amp i = j \end{cases} 
          </me>
        </p>
      </statement>
    </definition>
    <p>
      The Kronecker delta is a convenient short-hand to indicate that
      matching indices are important. The expressions
      <m>\delta_{5,6}</m>, <m>\delta_{4,10}</m> and
      <m>\delta_{9,3}</m> all evaluate to <m>0</m> since the indices
      do not match.  The expressions <m>\delta_{4,4}</m>,
      <m>\delta_{17,17}</m> and <m>\delta_{1,1}</m> all evaluate to
      <m>1</m> since the indices do match. 
    </p>
    <proposition>
      <statement>
        <p>
          Let <m>A</m> be an <m>n \times n</m> matrix. Write <m>A =
          (a_1, a_2, \ldots, a_n)</m> where the <m>a_i</m> are column
          vectors (for each <m>a_i</m> is a vector in<m>\RR^n</m>.
          Then the following seven statements are all equivalent, and
          all can be taken as the definition of an orthogonal matrix.
          <ol>
            <li>
              <p>
                <m>A</m> preserves lengths:
                <m>|v| = |Av|</m> for all <m>v \in \RR^n</m>.
              </p>
            </li>
            <li>
              <p>
                <m>A</m> preserves dot products:
                <m>u\cdot v = (Au) \cdot (Av)</m> for all <m>u</m>,
                <m>v \in \RR^n</m>.
              </p>
            </li>
            <li>
              <p>
                <m>A^T = A^{-1}</m>.
              </p>
            </li>
            <li>
              <p>
                <m>A^TA = \Id_n</m>.
              </p>
            </li>
            <li>
              <p>
                <m>a_i \perp a_j</m> for all
                <m>i \neq j</m> and <m>|a_i| = 1</m>.
              </p>
            </li>
            <li>
              <p>
                <m>a_i \cdot a_j = \delta_{ij}</m>
              </p>
            </li>
            <li>
              <p>
                <m>A</m> preserves angles (the angle between <m>u</m>
                and <m>v</m> is the same as the angle between
                <m>Au</m> and <m>Av</m> for all <m>u</m>, <m>v \in
                \RR^n</m>) <em>and</em> <m>\det A = \pm 1</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </proposition>
    <p>
      There are several very surprising statements here. First,
      preserving angles and preserving lengths are nearly equivalent
      (preserving angles needs the additional determinant condition).
      This is a little odd; there isn't a strong intuition that
      exactly the same transformations should preserve both
      properties. Second, there is a convenient algebraic method of
      identifying orthogonal matrices in property d): I can multiply
      by the transpose: if I get the identity, I know the matrix is 
      orthogonal. The equivalence of property d) and property f) is seen
      in simply calculating the matrix multiplication; it involves the
      dot products of all the columns, which must evaluate to <m>0</m>
      or <m>1</m> exactly matching the Kronecker delta.
    </p>
    <p>
      Orthogonal matrices can be thought of as rigid-body
      transformations. Since they preserve both lengths and angles,
      they preserve the shape of anything formed of vertices and
      lines: any polygon, polyhedron, or higher-dimensional analogue.
      They may not preserve the position (they may be moved around,
      rotated, reflected, etc.), but they will preserve the shape. This
      makes orthogonal matrices extremely useful since many
      applications want to use transformations that don't destroy
      polygons or polyhedra. Here is a proposition that gathers some
      other properties of orthogonal matrices. 
    </p>
    <proposition>
      <statement>
        <ul>
          <li>
            <p>
              In <m>\RR^2</m>, the only orthogonal transformations
              are the identity, the rotations and the reflections.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> is an orthogonal matrix, so is <m>A^{-1}</m>.
            </p>
          </li>
          <li>
            <p>
              All orthogonal matrices have determinant <m>\pm 1</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>A</m> and <m>B</m> are orthogonal matrices,
              then so are <m>AB</m> and <m>BA</m>.
            </p>
          </li>
        </ul>
      </statement>
      <proof>
        <p>
          The proofs of these statements will be part of the activity
          for this week.  
        </p>
      </proof>
    </proposition>
    <example>
      <statement>
        <p>
          Even in <m>\RR^3</m>, orthogonality already gets a bit trickier
          than just the rotations/reflection of <m>\RR^2</m>. Consider
          the following matrix. 
          <me> 
            \begin{pmatrix} 
              -1 \amp  0 \amp  0 \\ 
              0 \amp  -1 \amp  0 \\ 
              0 \amp  0 \amp  -1 
            \end{pmatrix}
          </me>
        </p>
        <p>
          This matrix is orthogonal, but it isn't a rotation or
          reflection in <m>\RR^3</m>. It is some kind of <sq>reflection
          through the origin</sq> where every point is sent to the
          opposite point with respect to the origin. (The equivalent
          in <m>\RR^2</m> is a rotation by <m>\pi</m> radians). It isn't
          even a physical transformation: it's not something I can do
          with a physical object without destroying it. (If the object
          were, say, an inflatable beach-ball, this transformation
          would be equivalent to collasping the ball, turning it
          inside-out, and reinflating it that way). However, the
          transformations still satisfies the condition of
          orthogonality.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Here is another example of an orthogonal matrix in
          <m>\RR^3</m>. 
          <me> 
            \begin{pmatrix} 
              \dfrac{5}{\sqrt{38}} \amp \dfrac{-3}{\sqrt{67}} 
                \dfrac{-23}{\sqrt{2546}} \\
              \dfrac{-3}{\sqrt{38}} \amp \dfrac{-7}{\sqrt{67}} 
                \dfrac{-9}{\sqrt{2546}} \\
              \dfrac{-2}{\sqrt{38}} \amp \dfrac{3}{\sqrt{67}} 
                \dfrac{-44}{\sqrt{2546}} \\
            \end{pmatrix}
          </me>
        </p>
        <p>
          To check the orthogonality, I would just take the dot
          products of the columns with each other. I should get
          <m>0</m> for each of these dot products to show that the
          columns are perpendicular to each other. Then I would check
          that the length of each column is <m>1</m>.
        </p>
      </statement>
    </example>
    <definition>
      <statement>
        <p>
          The group of orthogonal <m>n \times n</m> matrices is called
          the <term>Orthogonal Group</term> and is written
          <m>O_n(\RR)</m> or <m>O(n)</m>. The group of orthogonal
          matrices, which also preserve orientation (have determinant
          one) is called the <term>Special Orthogonal Group</term> and
          is written <m>SO_n(\RR)</m> or <m>SO(n)</m>.
        </p>
      </statement>
    </definition>
  </subsection>
  <!--
  
  Note: I removed this section but wanted to keep the typset notes
  incase I decided to include it again later.
 
 <subsection xml:id="subsection-orthogonal-bases">
    <title>Orthogonal Bases and the Gram-Schmidt Process</title>
    <definition>
      <statement>
        <p>
          A set of linearly independent non-zero vectors <m>\{ v_1,
          \ldots, v_k\}</m> is called an <term>orthogonal set</term>
          if <m>v_i \cdot v_j = 0</m> whenever <m>i \neq j</m>. The
          set is called an <term>orthonormal set</term> if <m>v_i
          \cdot v_j = \delta_{ij}</m>. If the set is a basis for a
          linear subspace, we use the terms <term>orthogonal
          basis</term> and <term>orthonormal basis</term>.
        </p>
      </statement>
    </definition>
    <p>
      In many ways, an orthonomal basis is the best kind of basis for
      a linear subspace: all the vectors are perpendicular to each
      other and all the vectors are unit vectors. The standard basis
      <m>\{e_1, \ldots, e_n\}</m> of <m>\RR^n</m> is the classic
      example of a orthonormal basis; all other orthonomal bases are
      trying to mimic the standard basis.
    </p>
    <p>
      Given a set of linearly independent vectors <m>\{v_1, \ldots,
      v_k\}</m> (usually a basis for a linear subspace), there is a
      general algorithm for adjusting them into an orthogonal or
      orthonomal basis. This algorithm is called the Gram-Schmidt
      algorithm. We define new vectors <m>u_i</m> as follows.
      <md>
        <mrow>
          u_1 \amp = v_1
        </mrow>
        <mrow>
          u_2 \amp = v_2 - \Proj_{u_1} v_2
        </mrow>
        <mrow>
          u_3 \amp = v_3 - \Proj_{u_1} v_3 - \Proj_{u_2} v_3
        </mrow>
        <mrow>
          u_4 \amp = v_4 - \Proj_{u_1} v_4 - \Proj_{u_2} v_4 -
          \Proj_{u_2} v_4
        </mrow>
        <mrow>
          \vdots \amp \vdots
        </mrow>
        <mrow>
          u_k \amp = v_k - \Proj_{u_1} v_k - \Proj_{u_2} v_k - \ldots
          - \Proj_{u_{k-1}} v_4
        </mrow>
      </md>
    </p>
    <p>
      Conceptually, this algorithm takes the first vector <m>v_1</m>
      as the first direction. Then in the second step, we take
      <m>v_2</m> and subtract any portion with lies in the <m>u_1</m>
      direction. Removing that commonality gives a perpendicular
      vector. In the third step, we take <m>v_3</m> and remove the
      projection onto the first two pieces. This again removes any
      commonality that we had so far, giving a perpendicular
      direction. We proceed in the same way through the list. The
      result is an orthogonal basis <m>\{u_1, \ldots, u_k\}</m>.
    </p>
    <p>
      The result is not, however, orthonormal. To get an orthonormal
      set, we divide each <m>u_i</m> by its length.
      <me>
        \left\{ \frac{u_1}{|u_i|}, \frac{u_2}{|u_2|}, \ldots,
        \frac{u_k}{|u_k|} \right\}
      </me>
    </p>
    <p>
      In this way, we can take any basis and turn it into an
      orthogonal or othonormal basis.
    </p>
  </subsection>
  -->
</section>
