<section xml:id="section-matrices-and-transformations">
  <title>Matrices and Transformations</title>
  <subsection xml:id="subsection-matrix-representation">
    <title>Matrix Representation of Linear Transformations</title>
    <p>
      Now I come to the second major application of matrices. In
      addition to succinctly encoding linear systems, matrices can
      also be used very efficiently to encode linear transformations.
      This is done by defining how a matrix can act on a vector.
    </p>
    <definition>
      <statement>
        <p>
          Let <m>A = a_{ij}</m> be a <m>m \times n</m> matrix and let
          <m>v</m> be a vector in <m>\RR^n</m>. There is an action of
          <m>A</m> on <m>v</m>, written <m>Av</m>, which defines a new
          vector in <m>\RR^m</m>. That action is given in the
          following formula.
          <me>
            \begin{pmatrix} a_{11} \amp a_{12} \amp \cdots
            \amp a_{1n} \\ a_{21} \amp a_{22} \amp \cdots \amp
            a_{2n} \\ \vdots \amp \vdots \amp \vdots \amp \vdots \\
            a_{n1} \amp a_{n2} \amp \cdots \amp a_{nn} \end{pmatrix}
            \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix}
            = \begin{pmatrix} a_{11}v_1 + a_{12}v_2 + \ldots +
            a_{1n}v_n \\ a_{21}v_1 + a_{22}v_2 + \ldots + a_{2n}v_n \\
            \vdots \\ a_{n1}v_1 + a_{n2}v_2 + \ldots + a_{nn}v_n
            \end{pmatrix}
          </me>
        </p>
        <p>
          This is a bit troubling to work out in general. Let me show
          what it looks like slightly more concretely in <m>\RR^2</m>
          and <m>\RR^3</m>.
          <md>
            <mrow>
              \begin{pmatrix}
              a \amp b \\ c \amp d  
              \end{pmatrix}
              \begin{pmatrix}
              x \\ y 
              \end{pmatrix} \amp = 
              \begin{pmatrix}
              ax + by \\ cx + dy 
              \end{pmatrix}
            </mrow>
            <mrow>
              \begin{pmatrix}
              a \amp b \amp c \\ d \amp e \amp f \\ g \amp h \amp i     
              \end{pmatrix}
              \begin{pmatrix}
              x \\ y \\ z 
              \end{pmatrix} \amp = 
              \begin{pmatrix}
              ax + by + cz \\ dx + ey + fz \\ gx + hy + iz 
              \end{pmatrix}
            </mrow>
          </md>
        </p>
        <p>
          In this way, all <m>m \times n</m> matrices determine a
          method of sending vectors in <m>\RR^n</m> to <m>\RR^m</m>: a
          function <m>\RR^n \rightarrow \RR^m</m>. It is not at all
          obvious from the definition, but matrices completely
          describe all linear transformations.
        </p>
      </statement>
    </definition>
    <proposition>
      <statement>
        <p>
          If <m>A</m> is an <m>m \times n</m> matrix, then the
          associated function defined by the matrix action is a linear
          function <m>\RR^n \rightarrow \RR^m</m>. Moreover, all
          linear functions <m>\RR^n \rightarrow \RR^m</m> can be
          encoded this way. Finally, each linear function is encoded
          uniquely, i.e., each <m>m \times n</m> matrix corresponds to
          a different transformation.
        </p>
      </statement>
    </proposition>
    <p>
      In this way, the set of linear transformations <m>\RR^n
      \rightarrow \RR^m</m> is <em>exactly</em> the same as the set of
      <m>m \times n</m> matrices. This is a very powerful result: in
      order to understand linear transformations of Euclidean space,
      I only have to understand matrices and their properties.
    </p>
    <p>
      I'll start with some important examples, using <m>3 \times 3</m>
      matrices for transformations from <m>\RR^3</m> to itself.
    </p>
    <example>
      <statement>
        <p>
          Let me interpret the matrix action of the zero matrix (all
          coefficient are zero).
          <me>
            \begin{pmatrix} 0 \amp 0 \amp 0 \\ 0 \amp 0 \amp 0 \\ 0
            \amp 0 \amp 0 \end{pmatrix} \begin{pmatrix} x \\ y \\ z
            \end{pmatrix} = \begin{pmatrix} 0x + 0y + 0z \\ 0x + 0y +
            0z \\ 0x + 0y + 0z \end{pmatrix} = \begin{pmatrix} 0 \\ 0
            \\ 0 \end{pmatrix}
          </me>
        </p>
        <p>
          The zero matrix corresponds to the transformation that sends
          all vectors to the origin.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Let me also interpret the matrix action of the identity
          matrix.
          <me>
            \begin{pmatrix} 1 \amp 0 \amp 0 \\ 0 \amp 1 \amp 0 \\ 0
            \amp 0 \amp 1 \end{pmatrix} \begin{pmatrix} x \\ y \\ z
            \end{pmatrix} = \begin{pmatrix} 1x + 0y + 0z \\ 0x + 1y +
            0z \\ 0x + 0y + 1z \end{pmatrix} = \begin{pmatrix} x \\ y
            \\ z \end{pmatrix}
          </me>
        </p>
        <p>
          The identity matrix corresponds to the transformation which
          doesn't change anything. Appropriately, we called this the
          identity transformation.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          I can also interpret a more general diagonal matrix. Let
          <m>a,b,c \in \RR</m> be scalars. 
          <me>
            \begin{pmatrix} a \amp 0 \amp 0 \\ 0 \amp b \amp 0 \\ 0
            \amp 0 \amp c \end{pmatrix} \begin{pmatrix} x \\ y \\ z
            \end{pmatrix} = \begin{pmatrix} ax + 0y + 0z \\ 0x + by +
            0z \\ 0x + 0y + cz \end{pmatrix} = \begin{pmatrix} ax \\
            by \\ cz \end{pmatrix}
          </me>
        </p>
        <p>
          This is a dilation: the <m>x</m> direction is stretched by
          the factor <m>a</m>, the <m>y</m> direction by the factor
          <m>b</m> and the <m>z</m> direction by the factor <m>c</m>.
          (If any of the three constants are zero, that entire axis
          direction is collapsed). Diagonal matrices are dilations in
          the axis directions, with the possibility of completely
          collapsing an axis as well. 
        </p>
      </statement>
    </example>
  </subsection>
  <subsection xml:id="subsection-matrix-multiplication">
    <title>Composition and Matrix Multiplication</title>
    <p>
      In <xref ref="subsection-composition" />, I defined the
      composition of linear transformations.
      Composition allows me to combine transformations.
      However, since matrices represent transformations, this
      composition should somehow be accounted for in the matrix
      representation. If <m>A</m> is the matrix of the transformation
      <m>S</m> and <m>B</m> is the matrix of the transformation
      <m>T</m>, what is the matrix of <m>S \circ T</m>? The answer is
      given by matrix multiplication.
    </p>
    <definition>
      <statement>
        <p>
          Let <m>A</m> be a <m>k \times m</m> matrix and <m>B</m> an
          <m>m \times n</m> matrix. I can think of the
          <term>rows</term> of <m>A</m> as vectors in <m>\RR^m</m>,
          and the <term>columns</term> of <m>B</m> as vectors in
          <m>\RR^m</m> as well. To emphasize this perspective, I
          write the following, using <m>u_i</m> for the rows of
          <m>A</m> and <m>v_i</m> for the columns of <m>B</m>.
          <md>
            <mrow>
            \amp A = \begin{pmatrix} \rightarrow \amp u_1 \amp
            \rightarrow \\ \rightarrow \amp u_2 \amp \rightarrow \\
            \rightarrow \amp u_3 \amp \rightarrow \\ \vdots \amp
            \vdots \amp \vdots \\ \rightarrow \amp u_k \amp
            \rightarrow \end{pmatrix} \amp \amp B = \begin{pmatrix}
            \downarrow \amp \downarrow \amp \downarrow \amp \ldots
            \amp \downarrow \\ v_1 \amp v_2 \amp v_3 \amp \ldots \amp
            v_n \\ \downarrow \amp \downarrow \amp \downarrow \amp
            \ldots \amp \downarrow \end{pmatrix}
            </mrow>
          </md>
        </p>
        <p>
          With this notation, the <term>matrix multiplication</term>
          of <m>A</m> and <m>B</m> is the <m>k \times n</m> matrix
          where the entries are the dot products of rows and columns.
          <me>
            AB = \begin{pmatrix} u_1 \cdot v_1 \amp u_1 \cdot
            v_2 \amp u_1 \cdot v_3 \amp \ldots \amp u_1 \cdot v_n \\
            u_2 \cdot v_1 \amp u_2 \cdot v_2 \amp u_2 \cdot v_3 \amp
            \ldots \amp u_2 \cdot v_n \\ u_3 \cdot v_1 \amp u_3 \cdot
            v_2 \amp u_3 \cdot v_3 \amp \ldots \amp u_3 \cdot v_n \\
            \vdots \amp  \vdots \amp  \vdots \amp  \ldots \amp  \vdots
            \\ u_k \cdot v_1 \amp u_k \cdot v_2 \amp u_k \cdot v_3
            \amp \ldots \amp u_k \cdot v_n \end{pmatrix} 
          </me>
        </p>
      </statement>
    </definition>
    <p>
      This operation has the desired property: the product of the
      matrices repesents the composition of the transformations.
      (This remarkable fact is presented here without proof; I'll
      leave it to you to wonder why this weird combination of dot
      products has the desired geometric interpretation.) Remember
      that the composition still works from right to left so that the
      matrix multiplication <m>AB</m> represents the transformation
      associated with <m>B</m> first, followed by the transformation
      associated with <m>A</m>. When I defined matrices acting on
      vectors, I wrote the action on the right: <m>Av</m>. Now when
      a composition acts, as in <m>ABv</m>, the closest matrix
      gets to act first.
    </p>
    <p>
      I have defined a new algebraic operation. As with the new
      products for vectors (dot and cross), I want to know the
      properties of this new operation.
    </p>
    <proposition xml:id="proposition-matrix-multiplication">
      <statement>
        <p>
          Let <m>A</m> be a <m>k \times l</m> matrix, <m>B</m> 
          an <m>l \times m</m> matrices, and <m>C</m> an <m>m
          \times n</m> matrix. Also, let <m>I_i</m> be the identity
          matrix in <m>\RR^i</m> for any <m>i \in \NN</m>. Then, there
          are two important properties of matrix multiplication I want
          to state at the start.
          <md>
            <mrow>
              A(BC) \amp = (AB) C \amp \amp \text{Associative}  
            </mrow>
            <mrow>
              AI_l \amp  = I_kA = A \amp \amp \text{Identity} 
            </mrow>
          </md>
        </p>
      </statement>
    </proposition>
    <p>
      Note that commutativity is not on this list. In general, <m>AB
      \neq BA</m>. In fact, if <m>A</m> and <m>B</m> are not square
      matrices, if <m>AB</m> is defined then <m>BA</m> will not be,
      since the indices will not match.  Not only am I unable to
      exchange the order of matrix multiplication, sometimes that
      multiplication doesn't even make sense as an operation. Matrix
      multiplication is a very important example of a non-commutative
      product.
    </p>
  </subsection>
</section>
