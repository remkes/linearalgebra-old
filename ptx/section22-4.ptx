<section>
  <title>Stochastic Matrices and Markov Chains</title>
  <p>
    Markov chains are probabalistic linear dynamical systems.
    Like all dynamical systems, we have a state vector <m>v \in \RR^n</m>.
    Here, <m>v</m> represents a probablistic state and the iterated process asks:
    what are the probabilities in the next state?
    Often, Markov chains are given graph theoretic interpretation.
    If <m>v \in \RR^n</m>, then we can consider a graph on <m>n</m> vertices.
    Each coefficient of the matrix <m>A_{ij}</m> is the probability of going from vertex <m>j</m> to vertex <m>i</m>.
    In terms of the graph,
    these transition number can be thought of as a label on a directed edge.
  </p>
  <p>
    In this interpretation,
    the <m>i</m>th column gives the total probabilities of all departures from the <m>i</m>th vertex
    (including <m>A_{ii}</m>, the probability of staying put).
    Since something must happen in each time step,
    we insist that the column sums,
    <m>r_j = \sum_i a_{ij}</m>, all must equal one.
  </p>
  <definition>
    <statement>
      <p>
        A non-negative <m>n \times n</m> matrix where all the columns sum to one is called a
        <term>(left) stochastic matrix</term>.
        (There is a similar definition of a righ stochastic matrix where all the rows sum to one;
        it is, in fact, the more common definition.
        However, it doesn't mesh that well with our conventions of columns vectors and matrix actions,
        so we will work with left stochastic matrices and surpress the word <sq>left</sq> in what follows.)
      </p>
    </statement>
  </definition>
  <p>
    Stochastic matrices reflect a probabalistic interpretation.
    They are are non-negative,
    so the weak form of Perron-Frobenius applies,
    but they may not be irreducible. (In this graph,
    this depends on whether or not each vertex is reachible from all the other vertices).
  </p>
  <p>
    As with previous iterated processes,
    we are curious about the long term behaviour.
    However, for stochastic matrices are are less directly concerned with eigenvalues and eigenvectors.
    (We can prove, easily from the column sums,
    that the dominant eigenvalue is
    <m>\lambda = 1</m> and the dominant eigenvector is the vector where all coefficients sum to 1.)
    Instead, we are more interested in the matrix <m>A^n</m> itself.
    The higher powers of the matrix calculate the iterated process.
    Therefore, we can interpret the coefficient
    <m>(A^k)_{ij}</m> as the probability of starting in vextex <m>j</m> and ending up in vertex <m>i</m> after <m>k</m> timesteps.
  </p>
</section>