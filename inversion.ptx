<section xml:id="section-inversion">
  <title>Inverse Linear Transforms and Matrix Inversion</title>
  <subsection xml:id="subsection-inverse-transforms">
    <title>Inverse Transforms</title>
    <p>
      I've now defined linear transformations. It is
      possible that a linear transformation is invertible, much like a
      function of numbers. Let me give the definition. 
    </p>
    <definition>
      <statement>
        <p>
          Let <m>T: \RR^n \rightarrow \RR^n</m>. The inverse
          transformation of <m>T</m> is a transformation <m>S: \RR^n
          \rightarrow \RR^n</m> which undoes what <m>T</m> did.
          Equivalently, it is a transformation <m>S</m> such that both
          the compositions <m>T \circ S</m> and <m>S \circ T</m> are
          the identity. For an arbitrary transformation, there is no
          guarantee that an inverse exists; if it does, it is written
          <m>T^{-1}</m>.
        </p>
      </statement>
    </definition>
    <p>
      Since I've encoded transformations as matrices, I can use
      matrices to understand the inverses of linear transformations.
    </p>
  </subsection>
  <subsection xml:id="subsection-inverse-matrices">
    <title>Inverse Matrices</title>
    <p>
      Before returning to inverse transformations, I'm going to talk
      about the algebraic idea of inversion and reciprocals.  In
      number systems, when there is an identity, there usually is a
      way of getting back to that identity. Zero is the identity for
      addition. For any number <m>a</m>, there exists a number
      <m>(-a)</m> so that <m>a + (-a) = 0</m> gets back to the
      identity (this obviously works in numbers systems that allow
      negatives, such as <m>\ZZ</m>). One is the identity in
      multiplication.  For any <em>non-zero</em> number <m>a</m>,
      there is the number <m>\frac{1}{a}</m> so that <m>a \frac{1}{a}
      = 1</m> gets back to the identity (this obviously works in
      numbers systems that allow reciprocals). For matrices, I have
      already defined the identity matrix, which is the identity for
      matrix multiplication. If <m>A</m> is a <m>k \times l</m>
      matrix, then <m>AI_l = I_k A = A</m>. For matrices, multiplying
      by the identity doesn't change the matrix.
    </p>
    <p>
      For matrices, I have the same question as I had for numbers
      systems. For any matrix <m>M</m>, is there another matrix
      <m>N</m> such that <m>MN = I</m>? Is there a way to get back to
      the identity?  Multiplication of numbers already showed that no
      such number need exist, since I cannot divide by zero. For
      matrices, I have to be even more cautious. For now, here is the
      formal definition.
    </p>
    <definition>
      <statement>
        <p>
          Let <m>M</m> be a <m>n \times n</m> (square) matrix. Then
          the <term>inverse</term> of <m>M</m> is the
          <term>unique</term> matrix <m>M^{-1}</m> (if it exists) such
          that <m>MM^{-1} = M^{-1}M = I_n</m>.
        </p>
      </statement>
    </definition>
    <p>
      I should note a couple of thing about this definition. First, it
      only applies to square matrices. I don't even try to invert
      non-square matrices. Second, both multiplications are required:
      <m>MM^{-1}</m> and <m>M^{-1}M</m>. Both are required because 
      matrix multiplication is non-commutative. In
      general, these two orders could result in different products. In
      this definition, I insist that both orders produce the 
      identity matrix.
    </p>
  </subsection>
  <subsection xml:id="subsection-calculation-inverse-matrices">
    <title>Calculating Inverse Matrices</title>
    <p>
      The definition is good, but I am left with the problem of
      determining which matrices have inverses and calculating those
      inverses. It turns out there is a convenient algorithm using
      techniques I have already presented. I will write an extended
      matrix <m>(M|I_n)</m> with the original matrix and the identity
      matrix next to each other, separated by a vertical line. Then I
      row reduce, treating the conglomeration as one matrix with long
      rows. If the row reduction of <m>M</m> results in the identity
      on the left, then the matrix has the form <m>(I|M^{-1})</m>; the
      right-hand side will be the inverse.
    </p>
    <p>
      This algorithm leads to two observations. First, obviously, it
      is a way to calculate inverses. But it also is a condition: an
      <m>n \times n</m> matrix is invertible, in this algorithm, only
      if it row reduces to the identity.  This is part of a general
      result, which is the first of several conditions for invertible
      matrices.
    </p>
    <proposition>
      <statement>
        <p>
          A <m>n \times n</m> matrix is invertible if and only if it
          row reduces to the identity matrix <m>I_n</m>. Since the
          identity has <m>n</m> leading ones, this is equivalent to
          the matrix having rank <m>n</m>.
        </p>
      </statement>
    </proposition>
    <definition xml:id="definition-general-linear">
      <statement>
        <p>
          The set of all intervible <m>n \times n</m> matrices with
          real coefficient forms a group. It is called the
          <term>General Linear Group</term> and written
          <m>GL_n(\RR)</m>. If I wanted to change the coefficients
          to another set of scalars <m>S</m>, I would write
          <m>GL_n(S)</m>. When the coefficients are understood, the
          notation is sometimes simplified to <m>GL_n</m> or
          <m>GL(n)</m>.
        </p>
      </statement>
    </definition>
  </subsection>
</section>
